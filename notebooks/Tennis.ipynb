{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Unity environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).\n",
    "\n",
    "__Before running the code cell below__, change the ENVIRONMENT_PATH parameter to match the location of the Unity environment that you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT_PATH = os.path.join(\"..\", \"environments\", \"Tennis.app\")\n",
    "#ENVIRONMENT_PATH = os.path.join(\"..\", \"environments\", \"Tennis_Linux\", \"Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "SRC_PATH = os.path.join(\"..\", \"src\")\n",
    "AGENT_CHECKPOINT_DIR = os.path.join(\"..\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(SRC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import UnityEnvWrapper\n",
    "from agents.policy_based import MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with UnityEnvWrapper(UnityEnvironment(file_name=ENVIRONMENT_PATH)) as env:\n",
    "    agent = MADDPG(\n",
    "        state_size=env.state_size, \n",
    "        action_size=env.action_size, \n",
    "        n_agents=n_agents,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    scores = agent.fit(\n",
    "        environment=env,\n",
    "        average_target_score=0.5,\n",
    "        agent_checkpoint_dir=AGENT_CHECKPOINT_DIR,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams[\"figure.figsize\"] = [9, 6]\n",
    "\n",
    "x = np.arange(len(scores))\n",
    "mu = pd.Series(scores).rolling(10).mean()\n",
    "std = pd.Series(scores).rolling(10).std()\n",
    "plt.plot(x, scores, linewidth=1)\n",
    "plt.plot(x, mu)\n",
    "plt.fill_between(x, mu+std, mu-std, facecolor=\"grey\", alpha=0.4)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "\n",
    "plt.savefig(\"scores\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pre-trained agent and play"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with UnityEnvWrapper(UnityEnvironment(file_name=ENVIRONMENT_PATH)) as env:\n",
    "    agent = MADDPG.load(AGENT_CHECKPOINT_DIR)\n",
    "    states = env.reset(train_mode=False)\n",
    "    scores = np.zeros(n_agents)\n",
    "    while True:\n",
    "        actions = agent.act(states)\n",
    "        next_states, rewards, dones = env.step(actions)\n",
    "        scores += rewards\n",
    "        states = next_states\n",
    "        if any(dones):\n",
    "            break\n",
    "    print(f\"Score: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
