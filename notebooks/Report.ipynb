{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5841a012",
   "metadata": {},
   "source": [
    "# Report of Project 3: Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef3e34",
   "metadata": {},
   "source": [
    "## Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f200d",
   "metadata": {},
   "source": [
    "The algorithm used in this project is a __Multi-Agent Deep Deterministic Policy Gradient__ (MADDPG) agent, based upon [Lowe et al.](https://arxiv.org/abs/1706.02275)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe28da",
   "metadata": {},
   "source": [
    "__The policies (actors)__ in MADDPG are fully-connected neural networks which output continuous values in a range of [-1, 1] for a given state input and are deterministic in nature. Therefore, to encourage exploration, especially in the beginning of training, we add __Gaussian noise__ $N_t$ to the actions generated by the neural networks, which we decay over time $t$ (episodes). That way, as with double DQN, we favor exploration in the beginning of the learning process and exploitation of the learned policy towards the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358d632",
   "metadata": {},
   "source": [
    "__The action-value functions (critics)__ are fully-connected neural networks similar to [Lillicrap et al.](https://arxiv.org/abs/1509.02971), but which differ in layer dimensions $(512, 256, 128)$ and using Leaky-ReLU activations. This architecture has been inspired from the [ddpg-bipedal](https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/model.py) example, provided by the authors of the Udacity's Deep Reinforcement Learning Nanodegree program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382b580",
   "metadata": {},
   "source": [
    "We use the same initialization scheme as proposed by [Lillicrap et al.](https://arxiv.org/abs/1509.02971), i.e. initializing the final layers of both the actor and critic with a uniform distribution $[−3×10^{−3},3×10^{−3}]$. The upstream layers were initialized with uniform distributions $[−1 \\frac{1}{√f},1 \\frac{1}{√f}]$ where $f$ is the fan-in of the respective layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867f185",
   "metadata": {},
   "source": [
    "As with the double DQN and DDPG case, each experience is stored in a fixed-size __experience replay buffer__ (a double-ended queue), from which the algorithm samples batches _uniformly at random_ to fit the neural networks, breaking the temporal correlation of experiences and thus generates i.i.d. samples from the buffer, which is expected by gradient-based __optimizers__ such as with __Adam__, which is used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcef16",
   "metadata": {},
   "source": [
    "__The model-fitting process__ (i.e. update of the neural network's weights) is done every 2-nd step of the MADDPG agent, using a sampled batch of experiences. The process - for all composite agents - is similar to double DQN, which is made up by four neural networks, i.e. two online networks for actor and critic which are continuously updated, and two target networks, which are used to generate the next step's actions and Q-values respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f380400",
   "metadata": {},
   "source": [
    "According to the algorithm of [Lowe et al.](https://arxiv.org/abs/1706.02275), we iterate over the composite agents and sample a random minibatch of $S$ samples $(\\textbf{x}^j, a^j,  r^j, \\textbf{x}^{'j})$ from the replay buffer, where $\\textbf{x}$ is a batch of concatenated observations $o_i$  from all participating agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393db180",
   "metadata": {},
   "source": [
    "The __critic__ is optimized via the next state's estimated actions and Q-values from the target networks of both agents and the local Q-value estimated from observations, $o_i$, and actions, $a_i = \\boldsymbol{\\mu_{\\theta_i}}(o_i) + N_t$, of all agents $i$. For the loss function, `SmoothL1Loss` a.k.a. Huber-Loss has been chosen, which is more robust to extreme values, i.e. mitigating very large gradients which potentially lead to oszillations and thus slow convergence, especially in the beginning of the learning process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5aa74",
   "metadata": {},
   "source": [
    "$$L(\\theta_i) = \\frac{1}{S}\\sum_j\\Big(y^j - Q^\\boldsymbol{\\mu}_i\\big(\\textbf{x}^j, a^j_1,...,a^j_N\\big)\\Big)^2, y^j = r^j_i+\\gamma Q^\\boldsymbol{\\mu^{'}}_i(\\textbf{x}^{'j},a^{'}_1,...,a^{'}_N)|_{a^{'}_k=\\boldsymbol{\\mu^{'}_k}(o^j_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93043040",
   "metadata": {},
   "source": [
    "The __actor__ is optimized via maximizing the expected value of the Q-network (critic), using the states and the policies selected actions of all composite agents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b0b6a",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\theta_i}J \\approx \\frac{1}{S} \\sum_j \\nabla_{\\theta_i}\\boldsymbol{\\mu}_i(o^j_i) \\nabla_{a_i} Q^{\\boldsymbol{\\mu}}_i(\\textbf{x}^j,a^j_1,...,a_i,...,a^j_N)|_{a_{i}=\\boldsymbol{\\mu}_i(o^j_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17094bbb",
   "metadata": {},
   "source": [
    "The final step in the model-fitting process is to \"soft-update\" the target network's weights. In contrast to overwriting the target-network weights with the online network weights every N time steps, we use __Polyak Averaging__, which updates the weights more often by mixing the weights with tiny bits of both networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b8e2b",
   "metadata": {},
   "source": [
    "$$\\theta_i^- = \\tau \\theta_i + (1-\\tau)\\theta_i^-$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160f732",
   "metadata": {},
   "source": [
    "As opposed to the previous projects, we used a pretty aggressive update of the target networks, using $\\tau=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c0ccd",
   "metadata": {},
   "source": [
    "Completing this fairly concise explanation of the algorithm, the following table provides a listing of hyper-parameters used for the final results presented in the next section:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98777977",
   "metadata": {},
   "source": [
    "Parameter Name | Value | Description\n",
    ":--- | --- | :---\n",
    "actor_hidden_layer_dimensions | (128, 64) | hidden layer dimensions of the policy network.\n",
    "critic_hidden_layer_dimensions | (512, 256, 128) | hidden layer dimensions of Q-network.\n",
    "activation_fn (critic)| Leaky-ReLU | the activation function used for the Q-network hidden layers.\n",
    "activation_fn (actor) | ReLU | the activation function used for the policy network.\n",
    "buffer_size | 1000_000 | replay buffer size.\n",
    "batch_size | 1024 | mini-batch size.\n",
    "gamma | 0.99 | discount factor.\n",
    "tau | 0.1 | interpolation parameter for target-network weight update.\n",
    "lr_actor | 1e-4 | learning rate of the policy network.\n",
    "lr_critic | 1e-4 | learning rate of the Q-value network.\n",
    "update_every | 2 | perform optimization every N steps.\n",
    "n_episodes | 5000 | maximum number of training episodes.\n",
    "max_t | 1000 |  maximum number of time steps per episode.\n",
    "eps_start | 1.0 | starting value of epsilon, for epsilon-greedy action selection.\n",
    "eps_end | 0.01 | minimum value of epsilon.\n",
    "eps_decay | 0.999 | multiplicative factor (per episode) for decreasing epsilon.\n",
    "scores_window_length | 100 | length of scores window to monitor convergence.\n",
    "average_target_score | 0.5 | average target score for scores_window_length at which learning stops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c399944",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cd4f8",
   "metadata": {},
   "source": [
    "The environment could be __solved in 2599 episodes__ using a local CPU environment:\n",
    "\n",
    "* OS: macOS Big Sur (Version 11.4)\n",
    "* Processors: 3,1 GHz Quad-Core Intel Core i7\n",
    "* Memory: 16 GB 2133 MHz LPDDR3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b45cbb",
   "metadata": {},
   "source": [
    "![Scores](scores.png \"Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323d950",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c76f4",
   "metadata": {},
   "source": [
    "From a modeling point of view, there are many ways to improve the algorithms and models even further. A (incomplete) listing of possible future improvements/extensions:\n",
    "\n",
    "* Try Bayesian variants of the given neural network architectures (e.g. Variational Inference or Bayesian approximation using Monte-Carlo Dropout), which could possibly improve decision making by incorporating (un)certainty based on posterior-predictive distributions\n",
    "* Utilize hyper-parameter optimization frameworks (hyperopt or scikit-optimize) rather than manual trial-and-error to further optimize the agent's settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65276415",
   "metadata": {},
   "source": [
    "## Additional References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b75bd",
   "metadata": {},
   "source": [
    "* [Lowe et al. \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments, arXiv (2020)\"](https://arxiv.org/abs/1706.02275)\n",
    "* [Lillicrap et al. \\\"Continuous Control with Deep Reinforcement Learning, ICLR (2016)\\\"](https://arxiv.org/abs/1509.02971)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba44096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
